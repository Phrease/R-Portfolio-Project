---
title: "Data Analyst Portfolio"
author: "Ryan Durkin"
date: "Updated: `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    theme: flatly
    toc: yes
    toc_float: yes
---

<br>

<center>
**[Denver, CO] | [858-349-4783] | [durkinrm@gmail.com] | [https://www.linkedin.com/in/ryan-durkin-1b93aaa2/] | [https://github.com/Phrease]**
</center>

<br>

### **Summary**

A highly motivated and detail-oriented data analyst with a passion for uncovering insights from data. Proficient in R for data manipulation, statistical analysis, and visualization. Seeking to leverage my analytical skills to solve complex problems and contribute to a data-driven organization.

---

### **Skills**

* **Programming Languages:** R (Advanced), SQL (Advanced), Python (Intermediate)
* **R Libraries:** `dplyr`, `ggplot2`, `tidyr`, `readr`, `shiny`, `rmarkdown`, `lubridate`
* **Data Visualization:** ggplot2, Tableau, Power BI
* **Databases:** PostgreSQL, MySQL, SQLite
* **Tools:** RStudio, Git & GitHub, Jupyter Notebooks

---

## **Project Showcase**

---

### **Project 1: Analyzing Colorado Counties via Rent Burden & Median Household Income**

* **Objective:** To analyze historical housing data for Colorado counties to identify trends in pricing, inventory, and sales volume over the last three years.

* **Description:** This project involved sourcing data from the ACS, going back to 2022 (with 5 year estimates), cleaning and wrangling the data using `dplyr`, and performing time-series analysis to identify seasonal patterns and long-term trends. The final output was an R Markdown report with interactive visualizations created using `ggplot2` and `tidycensus`.

* **Key Skills Demonstrated:** Data Cleaning, Data Wrangling, Time-Series Analysis, Data Visualization, R Markdown Reporting.

* **Code:** [Link to GitHub Repository for this Project](https://github.com/Phrease/R-Portfolio-Project/tree/main/analyses/colorado%20rent%20analysis)

<br>

```{r denver-housing-plot, echo=FALSE, fig.cap="Coloardo Counties Rent Burden compared to Median Household Income", out.width="100%"}
# Load the following libraries
library(tidycensus)
library(tidyverse)
library(sf)
library(ggplot2)
library(viridis)
library(scales)

# Access the Census Public Dataset for us income analysis
# Uncomment the below to load the api key
# census_api_key("22aeebd83dd72c7097e468b3963028eab9716cf8", install = TRUE)

# Load variables for the 5-year ACS for the most recent available year (e.g.; 2022)
# The 'cache = TRUE' argument makes the subsequent calls faster
acs_variables = load_variables(year = 2022, dataset = "acs5", cache = TRUE)

# Define the variables
# Define the variables for our analysis using descriptive names that we will use in our dataframe
my_variables <- c(
  median_gross_rent = "B25064_001",
  median_house_income = "B19013_001"
)

# Gross Rent as a Percentage of Household Income (B25070)
rent_burden_variables <- c(
  total_renter_occupied = "B25070_001",
  rent_lt_10_pct = "B25070_002",
  rent_10_14_9_pct = "B25070_003",
  rent_15_19_9_pct = "B25070_004",
  rent_20_24_9_pct = "B25070_005",
  rent_25_29_9_pct = "B25070_006",
  rent_30_34_9_pct = "B25070_007",
  rent_35_39_9_pct = "B25070_008",
  rent_40_49_9_pct = "B25070_009",
  rent_50_plus_pct = "B25070_010",
  # Units for which rent burden could not be computed
  rent_not_computed = "B25070_011"
)

# Combine all variables into a single list for get_acs()
all_my_variables <- c(my_variables, rent_burden_variables)

# Verify variables came through
print(all_my_variables)

# Get Median Gross Rent and Median Household Income by County for Colorado
# Download data at the county level for Colorado, using the 5-year ACS
colorado_data_raw <- get_acs(
  geography = "county",
  variables = all_my_variables,
  state = "CO",
  year = 2022,
  survey = "acs5",
  geometry = TRUE
)

# Inspect the first few rows and the structure of the downloaded data
print(head(colorado_data_raw))
print(glimpse(colorado_data_raw))

# Pivot data from long to wide format
colorado_data_wide <- colorado_data_raw %>%
  pivot_wider(
    names_from = variable,
    values_from = c(estimate, moe)
  )

# --- CRITICAL FIX: Temporarily remove the problematic 'agr' attribute (REVISED and CORRECTED) ---
# This prevents the 'internal error: can't find `agr` columns' during rename
colorado_data_cleaned <- colorado_data_wide %>%
  # Correct way to remove the 'agr' attribute within a pipe
  # This sets the 'agr' attribute of the sf object to NULL, bypassing the st_set_agr function's strict checks
  { attr(., "agr") <- NULL; . } %>%
  # Rename the columns for clarity (these names correspond to our 'all_my_variables' vector's names)
  # IMPORTANT: The column names from pivot_wider will be "estimate_your_variable_name"
  # and "moe_your_variable_name"
  rename(
    median_gross_rent = estimate_median_gross_rent,
    median_house_income = estimate_median_house_income,
    total_renter_occupied = estimate_total_renter_occupied,
    rent_lt_10_pct = estimate_rent_lt_10_pct,
    rent_10_14_9_pct = estimate_rent_10_14_9_pct, 
    rent_15_19_9_pct = estimate_rent_15_19_9_pct, 
    rent_20_24_9_pct = estimate_rent_20_24_9_pct, 
    rent_25_29_9_pct = estimate_rent_25_29_9_pct, 
    rent_30_34_9_pct = estimate_rent_30_34_9_pct, 
    rent_35_39_9_pct = estimate_rent_35_39_9_pct, 
    rent_40_49_9_pct = estimate_rent_40_49_9_pct, 
    rent_50_plus_pct = estimate_rent_50_plus_pct, 
    rent_not_computed = estimate_rent_not_computed,
    # Also rename the MOE columns
    moe_median_gross_rent = moe_median_gross_rent,
    moe_median_house_income = moe_median_house_income,
    moe_total_renter_occupied = moe_total_renter_occupied,
    moe_rent_lt_10_pct = moe_rent_lt_10_pct, 
    moe_rent_10_14_9_pct = moe_rent_10_14_9_pct, 
    moe_rent_15_19_9_pct = moe_rent_15_19_9_pct, 
    moe_rent_20_24_9_pct = moe_rent_20_24_9_pct, 
    moe_rent_25_29_9_pct = moe_rent_25_29_9_pct, 
    moe_rent_30_34_9_pct = moe_rent_30_34_9_pct, 
    moe_rent_35_39_9_pct = moe_rent_35_39_9_pct, 
    moe_rent_40_49_9_pct = moe_rent_40_49_9_pct, 
    moe_rent_50_plus_pct = moe_rent_50_plus_pct, 
    moe_rent_not_computed = moe_rent_not_computed
  ) %>%
  # Handle any potential NaN/Inf values that can arise from divisions by zero or NA values
  mutate(across(where(is.numeric), ~replace_na(., 0))) # This should now work on the renamed columns

# Calculate key rent burden metrics
colorado_data_final <- colorado_data_cleaned %>%
  mutate(
    computable_renters_total = total_renter_occupied - rent_not_computed,
    pct_rent_50_plus = (rent_50_plus_pct / pmax(1, computable_renters_total)) * 100,
    pct_rent_30_plus = ((rent_30_34_9_pct + rent_35_39_9_pct + rent_40_49_9_pct + rent_50_plus_pct) / pmax(1, computable_renters_total)) * 100,
    pct_rent_35_plus = ((rent_35_39_9_pct + rent_40_49_9_pct + rent_50_plus_pct) / pmax(1, computable_renters_total)) * 100
  ) %>%
  filter(computable_renters_total > 0)

# Inspect the transformed data
print(head(colorado_data_final))
print(glimpse(colorado_data_final))
print(colnames(colorado_data_final))

# Visualizie Rent Burden: Percentage 50% or more on rent
map_rent_50_plus <- ggplot(data = colorado_data_final) +
  geom_sf(aes(fill = pct_rent_50_plus), color = "white", lwd = 0.1) + # Set border color and thickness
  scale_fill_viridis_c(
    option = "magma",
    name = "% Severly Burdened", # Legend title
    labels = scales::label_percent(scale = 1), # Format as percentage
    direction = -1 # Reverse colors if desired (darker for higher values)
  ) +
  labs(
    title = "Colorado Counties: Rent Burden (50%+ of Income)",
    subtitle = "Percentage of Renter-Occupied Households Spending 50% or More on Rent",
    caption = "Sources: ACS 2022 5-Year Estimates | Data: tidycensus"
  ) +
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "right"
  )

# Print results
print(map_rent_50_plus)

# --- Visualize Median Household Income ---
map_median_income <- ggplot(data = colorado_data_final) +
  geom_sf(aes(fill = median_house_income), color = "white", lwd = 0.1) +
  scale_fill_viridis_c(
    option = "viridis",
    name = "Median Household Income",
    labels = scales::label_dollar(), # Format as dollar amount
    direction = 1 # Keep default direction
  ) +
  labs(
    title = "Coloardo Counties: Median Household Income",
    subtitle = "Estimated Median Household Income (2022 ACS)",
    caption = "Source: ACS 2022 5-Year Estimates | Data: tidycensus"
  ) +
  theme_minimal()
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "right"
  )

print(map_median_income)

# Calculate the Pearson correlation coefficient
correlation_coefficient <- cor(colorado_data_final$median_house_income,
                               colorado_data_final$pct_rent_50_plus,
                               use = "pairwise.complete.obs")

print(paste("Pearson Correlation Coefficient between Median Income and Percentage 50% Rent Burdened:", round(correlation_coefficient, 3), ". This correlation coefficient tells you that it's not simply 'richer counties have less severe rent burden' or 'poorer counties have more severe rent burden' in a straightforward, linear way. The situation is more nuanced and complex, driven by a blend of income distribution, actual housing costs, and other economic factors specific to each county."))
```

### **Project 2: Analyzing Colorado Tornadoes**

* **Objective:** To analyze historical storm data to identify trends in tornado frequency, intensity, and location within Colorado over the past five years.

* **Description:** This project used a dataset from the NOAA Storm Events Database. The data was cleaned and processed using dplyr and lubridate. Analysis focused on annual and monthly trends, intensity distributions (F/EF Scale), and geographic hotspots by county. The final report includes several visualizations from ggplot2 that highlight key findings.

* **Key Skills Demonstrated:** Data Cleaning, Data Wrangling, Time-Series Analysis, Geospatial Visualization, R Markdown.

* **Code:** [Link to GitHub Repository for this Project](https://github.com/Phrease/R-Portfolio-Project/tree/main/analyses/colorado%20weather%20patterns%20analysis)

<br>

```{r colorado-tornado-analysis, echo=FALSE, fig.cap="Analyizing Tornado Activity in Colorado in the Past 5 Year", out.width="100%"}
# Load the required packages
library(dplyr)
library(ggplot2)
library(lubridate)
library(readr)
library(forcats)
library(maps)
library(here)

# Data acquisition and loading
tornado_data <- read_csv(here("analyses", "datasets", "storm_data_search_results.csv"))

# Inspect the data
print(head(tornado_data))
print(str(tornado_data))
print(summary(tornado_data))

# Data cleaning and preperation
# Rename columns for easier use
tornado_data_clean <- tornado_data %>%
  rename(
    date = BEGIN_DATE,
    time = BEGIN_TIME,
    f_scale = TOR_F_SCALE,
    fatalities = DEATHS_DIRECT,
    injuries = INJURIES_DIRECT,
    property_damage = DAMAGE_PROPERTY_NUM,
    county = CZ_NAME_STR,
    latitude = BEGIN_LAT,
    longitude = BEGIN_LON,
    end_latitude = END_LAT,
    end_longitude = END_LON
  )

# Combine date and time to create a datetime column
# Ensure time is zero-padded to four digits (e.g., 17 -> 0017)
# Assuming 'time' is an integer, so we need to format it to a string with leading zeros if less than 4 digits
tornado_data_clean <- tornado_data_clean %>%
  mutate(
    time_str = sprintf("%04d", time), # Format time as 4-digit string with leading zeros
    date_time_start = mdy_hm(paste(date, time_str), quiet = TRUE) # Parse MM/DD/YYYY
  )

# Extract year and months
tornado_data_clean <- tornado_data_clean %>%
  mutate(
    year = year(date_time_start),
    month = month(date_time_start, label = TRUE, abbr = FALSE) # Full month name
  )

# Clean F/EF Scale: convert 'EFU' to NA and extract numeric part
# If TOR_F_SCALE is "EFU" it gets converted to NA. Otherwise, extract the digit after "EF"
tornado_data_clean <- tornado_data_clean %>%
  mutate(
    f_scale_numeric = as.numeric(gsub("EF", "", f_scale)) # Remove "EF" and convert to numeric
  ) %>%
  # Fill NA values in f_scale_number with MAGNITUDE if NAGNITUDE is numeric
  # NWS data sometimes uses MAGNITUDE for F/EF scale if TOR_F_SCALE is missing/EFU
  mutate(
    MAGNITUDE_numeric = as.numeric(MAGNITUDE), # Convert MAGNITUDE to numeric
    f_scale_numeric = ifelse(is.na(f_scale_numeric), MAGNITUDE_numeric, f_scale_numeric)
  )

# Filter for Colorado (STATE_ABBR should be 'CO')
tornado_data_clean <- tornado_data_clean %>%
  filter(STATE_ABBR == "CO")

# Remove rows with NA in critical columns for analysis
tornado_data_clean <- tornado_data_clean %>%
  filter(!is.na(year) & !is.na(f_scale_numeric))

# Tornado Analysis

# Annual Tornado Trends
annual_tornadoes <- tornado_data_clean %>%
  group_by(year) %>%
  summarise(count = n())

# Plot annual trends
p1 <- ggplot(annual_tornadoes, aes(x = year, y = count)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Annual Tornado Occurrences in Colorado",
    x = "Year",
    y = "Number of Tornadoes"
  ) +
  theme_minimal()
print(p1)
ggsave("annual_tornado_trends.png", width = 10, height = 6) # Save the plot

# Monthly Tornado Distribution (Seasonality)
monthly_tornadoes <- tornado_data_clean %>%
  group_by(month) %>%
  summarise(count = n()) %>%
  # Ensure months are in order
  mutate(month = fct_relevel(month, month.name))

# Plot monthly distribution
p2 <- ggplot(monthly_tornadoes, aes(x = month, y = count, fill = month)) +
  geom_bar(stat = "identity") + 
  labs(
    title = "Monthly Tornado Occurrences in Colorado",
    x = "Month",
    y = "Number of Tornadoes"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_viridis_d()
print(p2)
ggsave("monthly_tornado_distribution.png", plot = p2, width = 12, height = 7) # Save the plot

# Tornado Intensity Distribution (F/EF Scale)
f_scale_distribution <- tornado_data_clean %>%
  group_by(f_scale_numeric) %>%
  summarise(count = n()) %>%
  arrange(f_scale_numeric)

# Plot F/EF Scale distribution
p3 <- ggplot(f_scale_distribution, aes(x = factor(f_scale_numeric), y = count, fill = factor(f_scale_numeric))) +
  geom_bar(stat = "identity") +
  labs(
    title = "Tornado Intensity Distribution in Colorado (F/EF Scale)",
    x = "F/EF Scale Rating",
    y = "Number of Tornadoes",
    fill = "Rating"
  ) +
  theme_minimal() +
  scale_fill_viridis_d()
print(p3) # Display the plot
ggsave("tornado_intensity_distribution.png", plot = p3, width = 8, height = 6)

# Tornadoes by County
# Remove 'CO' suffix from county names for cleaner labels
tornado_data_clean <- tornado_data_clean %>%
  mutate(county_clean = gsub(" CO\\.", "", county))

top_counties <- tornado_data_clean %>%
  group_by(county_clean) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(10) # Get top 10 counties

# Plot top counties
p4 <- ggplot(top_counties, aes( x = reorder(county_clean, count), y = count, fill = county_clean)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Top 10 Colorado Counties by Tornado Occurrences",
    x = "County",
    y = "Number of Tornadoes"
  ) +
  coord_flip() + # Fill coordinate for better readability of county names
  theme_minimal() + 
  theme(legend.position = "none") +
  scale_fill_viridis_d()
print(p4) # Display the plot
ggsave("top_counties_tornadoes.png", plot = p4, width = 10, height = 7)

# Impact analysis (injuries, fatalities, property damage)
impact_by_f_scale <- tornado_data_clean %>%
  group_by(f_scale_numeric) %>%
  summarise(
    total_injuries = sum(injuries, na.rm = TRUE),
    total_fatalities = sum(fatalities, na.rm = TRUE),
    total_property_damage_billions = sum(property_damage, na.rm = TRUE) / 1e9 # Convert to billions
  ) %>%
  arrange(f_scale_numeric)

print(impact_by_f_scale)

# Plot fatalities by tornado intensity
p5 <- ggplot(impact_by_f_scale, aes(x = factor(f_scale_numeric), y = total_fatalities, fill = factor(f_scale_numeric))) +
  geom_bar(stat = "identity") +
  labs(
    title = "Fatalities by Tornado Intensity (F/EF Scale) in Colorado",
    x = "F/EF Scale Rating",
    y = "Total Fatalities",
    fill = "Rating"
  ) +
  theme_minimal() +
  scale_fill_viridis_d()
print(p5)
ggsave("fatalities_by_f_scale.png", plot = p5, width = 8, height = 6)

# Plot property damage by Tornado Intensity
p6 <- ggplot(impact_by_f_scale, aes(x = factor(f_scale_numeric), y = total_property_damage_billions, fill = factor(f_scale_numeric))) +
  geom_bar(stat = "identity") +
  labs(
    title = "Property Damage by Tornado Intenisty (F/EF Scale) in Colorado",
    x = "F/EF Scale Rating",
    y = "Total Property Damage (Billions USD)",
    fill = "Rating"
  ) +
  theme_minimal() +
  scale_fill_viridis_d()
print(p6)
ggsave("property_damage_by_f_scale.png", plot = p6, width = 8, height = 6)

# Spatial Analysis: Tornado Touchdown Locations and Paths
# Using base map of Colorado, we may need 'maps' package or 'sf' for more detailed maps
library(maps)
colorado_map <- map_data("county", region = "colorado")

p7 <- ggplot() +
  geom_polygon(data = colorado_map, aes(x = long, y = lat, group = group), fill = "white", color = "gray") +
  geom_point(data = tornado_data_clean, aes(x = longitude, y = latitude, color = f_scale_numeric), alpha = 0.6) +
  scale_color_viridis_c(option = "magma", direction = -1, name = "F/EF Scale") +
  scale_size_continuous(range = c(1, 8), name = "F/EF Scale") + # Adjust size range for better visualization
  labs(
    title = "Colorado Tornado Touchdown Locations by Intensity",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_void() +
  coord_fixed(1.3)
print(p7)
ggsave("tornado_touchdown_locations.png", plot = p7, width = 10, height = 8)

# For tornado paths, filter for valid start and end coordinates
tornado_paths_data <- tornado_data_clean %>%
  filter(!is.na(latitude) & !is.na(longitude) & !is.na(end_latitude) & !is.na(end_longitude))

if(nrow(tornado_paths_data) > 0) {
  p8 <- ggplot() +
    geom_polygon(data = colorado_map, aes(x = long, y = lat, group = group), fill = "white", color = "gray") +
    geom_segment(data = tornado_paths_data, aes(x = longitude, y = latitude, xend = end_longitude, yend = end_latitude, color = f_scale_numeric),
                 arrow = arrow(length = unit(0.02, "npc")), linewidth = 0.5, alpha = 0.7) +
    scale_color_viridis_c(option = "magma", direction = -1, name = "F/EF Scale") +
    labs(
      title = "Colorado Tornado Paths by Intensity",
      x = "Longitude",
      y = "Latitude"
    ) +
    theme_void() +
    coord_fixed(1.3)
  print(p8)
  ggsave("tornado_paths_approximation.png", plot = p8, width = 10, height = 8)
} else {
  print("No complete tornado path data (missing start/end coordinates) to plot paths.")
}
```

### **Project 3: Analyzing Asteroid Densities by their Weighted Mean**

* **Objective: To calculate a more accurate central tendency for asteroid densities by applying a weighted mean that accounts for measurement uncertainty.

* **Description: This analysis uses the asteroid_dens dataset from the astrodatR package. Instead of a simple average, a weighted mean was calculated, giving more influence to measurements with lower uncertainty (i.e., higher precision). This provides a more robust estimate of the true average density of the asteroid population in the sample.

* **Key Skills Demonstrated:** Statistical Analysis, Weighted Means, Data Visualization, R Markdown Reporting.

* **Code:** [Link to GitHub Repository for this Project](https://github.com/Phrease/R-Portfolio-Project/tree/main/analyses/asteroid%20density%20analysis)

<br>

```{r asteroid-density-weight, echo=FALSE, fig.cap="Asteroid Density Measured by Weighted Mean", out.width="100%"}
# To perform a data analysis on the astrodatR package, we need go into the respository and download it from there as this dataset has been deprecated
# Install removes library to access the dataset
library(remotes)

# Install astrodatR from the CRAN archive
# The version 0.1 was the last one available before archiving
install_version("astrodatR", version = "0.1", repos = "http://cran.us.r-project.org")

# Now we can load the astrodatR library
library(astrodatR)

# Access the asteroid_dens dataset
data("asteroid_dens")

# Rename our columns for easier interpretation
asteroid_dens <- asteroid_dens %>%
  rename(
    name = Asteroid,
    density = Dens,
    unc = Err
  )

# For this analysis, we'll be looking at the Weighted Mean and Robust Estimation
# Load the following libraries
library(ggplot2)
library(dplyr)

# Explore the basic statistics
cat("--- Basic Statistics ---\n")
mean_density <- mean(asteroid_dens$density)
median_density <- median(asteroid_dens$density)
sd_density <- sd(asteroid_dens$density)

cat("Simple Mean Density:", round(mean_density, 3), "g/cm^3\n")
cat("Median Density:", round(median_density, 3), "g/cm^3\n")
cat("Standard Deviation of Densities:", round(sd_density, 3), "g/cm^3\n")

# Calculate the weighted mean density
# Weights are inversely proportional to the variance (uncertainty squared)
# w_i = 1 / sigma_i^2
# X_w = sum(w_i * x_i) / sum(w_i)
asteroid_dens <- asteroid_dens %>%
  mutate(variance = unc^2,          # Calculate variance from uncertainty
         weight = 1 / variance)     # Calculate weight

weighted_mean_density <- sum(asteroid_dens$density * asteroid_dens$weight) / sum(asteroid_dens$ weight)
cat("Weighted Mean Density (accounting for uncertainty):", round(weighted_mean_density, 3), "g/cm^r\n")

# Calculate the standard error of the weighted mean
se_weighted_mean <- sqrt(1 / sum(asteroid_dens$weight))
cat("Standard Error of the Weighted Mean:", round (se_weighted_mean, 3), "g/cm^r\n")

cat("\n--- Precision Analysis ---\n")
# Sort by uncertainty to see which measurements are more/least precise
asteroid_dens_sorted_unc <- asteroid_dens %>%
  arrange(unc)

cat("Top 5 Most Precisely Measure Asteroids:\n")
print(head(asteroid_dens_sorted_unc, 5) %>% select(name, density, unc))

cat("Top 5 Least Precisely Measure Asteroids:\n")
print(tail(asteroid_dens_sorted_unc, 5) %>% select(name, density, unc))

# --- Visualizing the Weighted Mean and Distribution ---

# Re-create the density plot with error bars, and add the weighted mean
asteroid_dens_ordered <- asteroid_dens %>%
  arrange(density) %>%
  mutate(asteroid_name_factor = factor(name, levels = name))

ggplot(asteroid_dens_ordered, aes(x = asteroid_name_factor, y = density)) +
  geom_pointrange(aes(ymin = density - unc, ymax = density + unc), color = "darkblue") +
  geom_hline(yintercept = weighted_mean_density, linetype = "dashed", color = "red", size = 0.8) +
  annotate("text", x = 1, y = weighted_mean_density + 0.2, label = paste("Weighted Mean:", round(weighted_mean_density)), color = "red", hjust = 0) +
  labs(title = "Asteroid Densities with Measurement Uncertainties and Weighted Mean",
       x = "Asteroid",
       y = "Density (g/cm^3)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

# Histogram of the weighted mean
ggplot(asteroid_dens, aes(x = density)) +
  geom_histogram(binwidth = 0.2, fill = "skyblue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = weighted_mean_density, linetype = "dashed", color = "red", size = 1) +
  labs(title = "Distribution of Asteroid Densities with Weighted Mean",
       x = "Density (g/cm^3)",
       y = "Frequency") +
  theme_minimal() +
  annotate("text", x = weighted_mean_density + 0.5, y = max(hist(asteroid_dens$density, breaks=seq(min(asteroid_dens$density), max(asteroid_dens$density)+0.2, by=0.2), plot=FALSE)$counts) * 0.9,
           label = paste("Weighted Mean:", round(weighted_mean_density, 2)), color = "red", hjust = 0)

# Investigate potential outliers (Qualitative, given small N)
asteroid_dens_deviations <- asteroid_dens %>%
  mutate(deviation_from_wmean = abs(density - weighted_mean_density),
         deviation_in_unc = deviation_from_wmean / unc) %>%
  arrange(desc(deviation_in_unc))

cat("\n--- Asteroids with Largest Deviations (in terms of their uncertainty) ---\n")
print(head(asteroid_dens_deviations, 5) %>%
        mutate(w_mean = weighted_mean_density) %>%
        select(name, density, unc, w_mean, deviation_in_unc))
```